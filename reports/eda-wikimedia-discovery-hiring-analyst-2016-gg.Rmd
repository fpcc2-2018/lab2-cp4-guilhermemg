---
title: "Wikimedia-Discovery-Hiring-Analyst-2016"
author: "Guilherme Gadelha"
date: "April 24, 2018"
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    df_print: kable
    theme: united
    highlight: tango
---

# [Setup](#setup)

```{r setup, include=FALSE}

library(tidyverse)
library(lubridate)
library(here)

searches = read_csv(here::here("data/search_data.csv"))

head(searches)
names(searches)

```

To make this analysis we modified the original script that gets the data from wikimedia. The modifications are the following:

1. Original data slice size: 100000 observations (logged events)
2. Change of variable name from search_index to num_searches
3. Change of variable name from session_start_timestamp to first_event_timestamp
4. Change of variable name from session_start_date to first_event_date_time

The changes were made for improve the clarity and fidelity to the original dataset.

# [Question 1](#question1)
####*What is our daily overall clickthrough rate? How does it vary between the groups?*

## [Clean Data](#clean1)

We are interested in to calculate the daily overrall clickthrough rate. However, we have two particular problems:

1. In the same search session, the user can do multiple searches, so multiple search events are recorded associated with the same *session_id*.
2. The logged days do not correspond to intervals of exactly 24h to all the nine days in the dataset, we
need to balance the distribution of events over the days considering this fact.

### [Method](#clean1Method)

In order to clean and summarize the data we are dealing with, the following procedure was executed:

1. a new variable *day* was created to record the day of the session;
2. a grouping was made by session, day and group, summarizing the sum of clicks (*visitPage* events) by each row in the final dataset;

```{r clean_1}
sessions <- searches %>%
  mutate(hour = round_date(first_event_date_time, unit="hour")) %>% 
  group_by(session_id, hour, group) %>% 
  summarize(n_clicks = sum(num_clicks),
            n_sessions = n())

sessions %>% head()
```

A special care with sessions that could last from one day to another was observed, so we checked if there was any repeated *session_id*, but there was any.

```{r}
sessions %>% 
  distinct(session_id, .keep_all = T) %>% 
  dim()

sessions %>% 
  dim()
```

Grouping by session, we solve the first problem pointed out (same session with multiple searches in it).

For solve the second problem, we need to identify the days that have an incomplete log spectrum, i.e., the ones do not catch data in a 24 hour period corresponding to that day.

```{r}
n_sessions_df <- sessions %>% 
  group_by(hour, group) %>% 
  summarise(n_sessions = sum(n_sessions))

n_sessions_df %>% head()

n_sessions_df %>% 
  ggplot(aes(x = hour, y = n_sessions, color=group)) +
  geom_line() +
  geom_point() +
  facet_grid(~ group) +
  labs(x = "Hour",
       y = "Number of Sessions",
       colour = "Group")
```

As we can see in the figure below, the last day has missing hours, then we will filter it from our dataset, so we can calculate the daily clickthrough rate without any missing data.

```{r}
sessions <- sessions %>% 
  filter(day(hour) != 8)

sessions %>% dim()
```


Calculating and plotting the daily clickthrough rate (DCR)
```{r}

dcr_df <- sessions %>% 
  mutate(day = day(hour)) %>%
  group_by(day, group) %>%
  summarise(dcr = sum(n_clicks)/sum(n_sessions))

dcr_df %>% head()

dcr_df %>% 
  ggplot(aes(x = day, y = dcr, color=group)) +
  geom_point() +
  geom_line() +
  # geom_hline(aes(yintercept = mean(dcr_df$dcr), linetype="Mean"), colour="red") +
  facet_grid(~ group) +
  labs(x = "Day",
       y = "Clickthrough Rate",
       color="Group")
       # linetype="Line Type")
```

The daily overrall clickthrough rate (DCR): _*the proportion of search sessions where the user clicked on one of the results displayed*_. 

For the group *a*, on the first four days the DCR is greather than or equal 0.40, that means around 40% of search sessions resulted in at least one click by the user with the intention of visiting a page listed on a SERP (search engine result page) returned to him after a search query.

It is important to make an observation: a good search engine returns to the user the best result in the firsts positions in the SERP, so he needs to click only once in the page and trigger an action of _visitPage_, logged by the wikimedia servers. In the following days, the clickthrough rate decreases,  staying around 0.30.

For the group *b*, the clickthrough rate is visibly smaller than the observed in the group *a*, being below 0.2 for all the days in our sample.


# [Question 2](#question2)
####*Which results do people tend to try first? How does it change day-to-day?*

## [Clean Data]({#clean2})

Here we are interested in *visitPage* events that happened after a search. The *visitPage* events are recorded in the *num_clicks* variable and the *position* of the result clicked first in the SERP (search engine result page) is recorded in *first_click* variable.

As identified on the previous sections, we have incomplete logged data for all the days, so we will exclude data from the last day (March 8th), so we can draw conclusions about the entire day and have some baseline to compare the days.

It is of special interest to us to know in what result people first clicked, then we need to filter the events that have a number of clicks (*num_clicks*) greather than zero.

```{r clean_2}
searches <- searches %>%
  mutate(day = day(first_event_date_time)) %>% 
  filter(day != 8 & num_clicks >= 1 & first_click != 'NA')
```

It is important observe too that we are no more interested on the data on a _session level_, but in a _search level_ now. 

Next we analyse how the position of first result clicked on the SERP vary by day and by group. To do that we check two metrics: the mean and median values to the *first click* variable.

```{r}

aux1 <- searches %>% 
  group_by(day, group) %>%
  summarise(mean_first_click = mean(first_click),
            med_first_click = median(first_click))

aux1 %>% 
  ggplot(aes(x = day, y = mean_first_click)) +
  geom_col() +
  facet_grid(~group) +
  labs(x = "Day",
       y = "Mean First Click",
       title="Mean of First Clicks By Day and Group")

aux1 %>% 
  ggplot(aes(x = day, y = med_first_click)) + 
  geom_col() +
  facet_grid(~group) +
  labs(x = "Day",
       y = "Median First Click",
       title = "Median of First Click by Day and Group")
```

The visualizations show us that the extreme values highly influentiate the mean, we can see for the days 1 and 4 that the mean has very high if compared with others days.

So we analyse the median as a more representative metric of evaluation, however, the second visualization show us that all days in both groups have median value of 1 and then we cannot make any other valuable conclusion.

In face of this problem, we need to concentrate our attention on these extreme values, once they are influencing the analysis and we cannot get rid of them. So we analize the distribution of the first_click values:

```{r}
searches %>% 
  filter(first_click > 0) %>% 
  ggplot(aes(x = first_click)) +
  geom_histogram() +
  scale_y_log10() +
  labs(title = "Histogram of First Click",
       x = "First Click",
       y = "Count")
```




```{r}
aux2 <- searches %>% 
  group_by(day, group) %>%
  summarise(max_first_click = max(first_click))

aux2 %>%
  ggplot(aes(x = day, y = max_first_click)) +
  geom_col() +
  facet_grid(~group) +
  labs(x = "Day",
       y = "Max First Click")
```

