---
title: "Wikimedia-Discovery-Hiring-Analyst-2016"
author: "[Guilherme Gadelha](https://www.linkedin.com/in/guilhermegadelha/)"
date: "April 24, 2018"
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float: 
        collapsed: false
        smooth_scroll: false
    df_print: kable
    theme: cerulean
    highlight: tango
---

<style>
body 
{
  text-align: justify
}
</style>

# [Setup](#setup)

```{r setup, include=FALSE}

library(tidyverse)
library(lubridate)
library(here)
library(grid)

searches = read_csv(here::here("data/search_data.csv"))

head(searches)
names(searches)

```

To make this analysis we modified the original script that gets the data from wikimedia. The modifications are the following:

1. Change of variable name from search_index to num_searches
2. Change of variable name from session_start_timestamp to first_event_timestamp
3. Change of variable name from session_start_date to first_event_date_time

The changes were made for improve the clarity and fidelity to the original dataset.

# [Question 1](#question1)
####*What is our daily overall clickthrough rate? How does it vary between the groups?*

## [Cleaning Data](#clean1)

We are interested in to calculate the daily overrall clickthrough rate. However, we have two particular problems:

1. In the same search session, the user can do multiple searches, so multiple search events are recorded associated with the same *session_id*.
2. The logged events do not correspond to intervals of exactly 24h to all the days in the dataset, we
need to balance the distribution of events over the days considering this fact or exclude the day(s) with missing events in our analysis.

### [Method](#clean1Method)

In order to clean and summarize the data we are dealing with, the following procedure was executed:

1. a new variable *hour* was created to record the session starting hour;
2. a grouping was made by session, hour and group, summarizing the sum of clicks (*visitPage* events) by each row in the final dataset;

```{r clean_11}
sessions <- searches %>%
  mutate(hour = round_date(first_event_date_time, unit="hour")) %>% 
  group_by(session_id, hour, group) %>% 
  summarize(n_clicks = sum(num_clicks),
            n_sessions = n())

sessions %>% head()
```

A special care with sessions that could last from one day to another was observed, so we checked if there was any repeated *session_id*, but there was any.

```{r clean_12}
sessions %>% 
  distinct(session_id, .keep_all = T) %>% 
  dim()

sessions %>% 
  dim()
```

Grouping by session, we solve the first problem pointed out (same session with multiple searches in it).

For solve the second problem, we need to identify the days that have an incomplete log spectrum, i.e., the ones do not catch data in a 24 hour period corresponding to that day.

```{r clean_13, fig.align='center'}
n_sessions_df <- sessions %>% 
  group_by(hour, group) %>% 
  summarise(n_sessions = sum(n_sessions))

n_sessions_df %>% head()

n_sessions_df %>% 
  ggplot(aes(x = hour, y = n_sessions, color=group)) +
  geom_line() +
  geom_point() +
  facet_grid(~ group) +
  labs(x = "Hour",
       y = "Number of Sessions",
       colour = "Group")
```

As we can see in the figure below, the last day has missing hours, then we will filter it from our dataset, so we can calculate the daily clickthrough rate without any missing data.

```{r clean_14}
sessions <- sessions %>% 
  filter(day(hour) != 8)

sessions %>% dim()
```

## [Calculating](#calculating1)
Calculating and plotting the daily clickthrough rate (DCR)
```{r calc1}

dcr_df <- sessions %>% 
  mutate(day = day(hour)) %>%
  group_by(day, group) %>%
  summarise(dcr = sum(n_clicks)/sum(n_sessions))

dcr_df %>% head()

dcr_df %>% 
  ggplot(aes(x = day, y = dcr, color=group)) +
  geom_point() +
  geom_line() +
  labs(x = "Day",
       y = "Clickthrough Rate",
       color="Group")
```

The daily overrall clickthrough rate (DCR): _*the proportion of search sessions where the user clicked on one of the results displayed*_. 

For the group *a*, on the first four days the DCR is greather than or equal 0.40, that means around 40% of search sessions resulted in at least one click by the user with the intention of visiting a page listed on a SERP (search engine result page) returned to him after a search query.

It is important to make an observation: a good search engine returns to the user the best result in the firsts positions in the SERP, so he needs to click only once in the page and trigger an action of _visitPage_, logged by the wikimedia servers. In the following days, the clickthrough rate decreases,  staying around 0.30.

For the group *b*, the clickthrough rate is visibly smaller than the observed in the group *a*, being below 0.2 for all the days in our sample.


# [Question 2](#question2)
####*Which results do people tend to try first? How does it change day-to-day?*

## [Cleaning Data]({#clean2})

Here we are interested in *visitPage* events that happened after a search. The *visitPage* events are recorded in the *num_clicks* variable and the *position* of the result clicked first in the SERP (search engine result page) is recorded in *first_click* variable.

As identified on the previous sections, we have incomplete logged data for all the days, so we will exclude data from the last day (March 8th), so we can draw conclusions about the entire day and have some baseline to compare the days.

It is of special interest to us to know in what result people first clicked, then we need to filter the events that have a number of clicks (*num_clicks*) greather than zero.

```{r clean_21}
searches2 <- searches %>%
  mutate(day = day(first_event_date_time)) %>% 
  filter(day != 8 & num_clicks >= 1 & first_click != 'NA' & first_click > 0)
```

It is important observe too that we are no more interested on the data on a _session level_, but in a _search level_ now. 

Next we analyse how the position of first result clicked on the SERP vary by day and by group. To do that we check two metrics: the mean and median values to the *first click* variable.

## [Calculating](#calculating2)
```{r calc_21}

aux1 <- searches2 %>% 
  group_by(day, group) %>%
  summarise(mean_first_click = mean(first_click),
            med_first_click = median(first_click))

g1 = aux1 %>% 
  ggplot(aes(x = day, y = mean_first_click)) +
  geom_col() +
  facet_grid(~group) +
  labs(x = "Day",
       y = "Mean First Click",
       title="Mean of First Clicks By Day and Group") +
  ylim(0, 10)

g2 = aux1 %>% 
  ggplot(aes(x = day, y = med_first_click)) + 
  geom_col() +
  facet_grid(~group) +
  labs(x = "Day",
       y = "Median First Click",
       title = "Median of First Click by Day and Group") +
  ylim(0, 10)

grid.newpage()
grid.draw(rbind(ggplotGrob(g1), ggplotGrob(g2), size="last"))
```

The visualizations show us that the extreme values highly influentiate the mean, we can see for the days 1 and 4 of *group a* that the mean is very high if compared with the others days.

So we analyse the median as a more representative metric of evaluation, however, the second visualization show us that all days in both groups have median value of 1 and then we cannot make any other valuable conclusion from it.

In face of this problem, we need to concentrate our attention on the extreme values, once they are influencing the analysis and we cannot get rid of them. So we analyze the distribution of the *first_click* values:

```{r calc_22, warning=FALSE, fig.align='center'}
searches2 %>% 
  filter(first_click > 0) %>% 
  ggplot(aes(x = first_click)) +
  geom_histogram(binwidth = 80) +
  scale_y_log10() +
  labs(title = "Histogram of First Click",
       x = "First Click",
       y = "Count")
```


Looking at the histogram of first click values, we see clearly it is right skewed and has many values concentrated around 1 click, asserting the previous analysis with the mean and the median. However other values can be explored:


```{r calc_23, fig.align='center'}
searches2 %>% 
  mutate(first_click_group = ifelse(first_click == 1, "A (==1)", 
                                    ifelse(first_click <= 5, "B (1< & <=5)",
                                           ifelse(first_click <= 10, "C (>5 & <= 10)",
                                                  ifelse(first_click < 20, "D (>10 & <= 20)",
                                                         "E (>20)"))))) %>% 
  group_by(group, day, first_click_group) %>% 
  summarise(n_first_click_group = n()) %>% 
  ggplot(aes(x = first_click_group, y = n_first_click_group)) +
  geom_point(aes(color = group)) + 
  facet_grid(day ~ .) +
  labs(main = "Group of First Clicks (A-E) by Day (1-7)",
         y = "Size of First Click Group",
         x = "First Click Group",
         color = "Dataset Group")
```

The visualization allow us to see that the *group a* has a better performance for every analyzed day, considering that how many more first clicks better and how lower the position of the first click too. The groups A through E represents a position or set of positions the first click can assume, so, for example, to A (== 1), the first click is on first position and for B (1< & <= 5) the first click is between the second and fifth postions.


# [Question 3](#question3)
####*What is our daily overall zero results rate? How does it vary between the groups?*

## [Cleaning Data]({#clean3})

We are interested in to calculate the daily zero results rate (ZRR). The daily ZRR can be defined like that: the proportion of searches that yielded 0 results in a day-to-day interval.

For this question we face a familiar problem:

1. The logged events do not correspond to intervals of exactly 24h to all the days in the dataset, so we need to balance the distribution of events over the days considering this fact or exclude the day(s) with missing events in our analysis.

### [Method](#cleanMethod3)

In order to clean and summarize the data we are dealing with, the following procedure was executed:

1. a new variable *day* was created to record the day of the session;
2. we filtered the events of the eight day, that is incomplete;

```{r clean3}
searches3 <- searches %>% 
  mutate(day = day(first_event_date_time)) %>% 
  filter(day != 8)
```


## [Data Exploration](#eda3)

Lets explore the variable *results* from our dataset that records the number of results returned in the search event. 

```{r calc_31}
g1 = searches3 %>% 
  filter(results == 0) %>% 
  group_by(day, group) %>% 
  summarise(zero_results = n()) %>% 
  ggplot(aes(x = day, y = zero_results, color=group)) +
  geom_point() +
  geom_line() +
  labs(x = "Day",
       y = "Amount of Zero Results",
       color = "Group") +
  ylim(0, 12000)

g2 = searches3 %>% 
  filter(results != 0) %>% 
  group_by(day, group) %>% 
  summarise(zero_results = n()) %>% 
  ggplot(aes(x = day, y = zero_results, color=group)) +
  geom_point() +
  geom_line() +
  labs(x = "Day",
       y = "Amount of Non-Zero Results",
       color = "Group") +
  ylim(0, 12000)

grid.newpage()
grid.draw(rbind(ggplotGrob(g1), ggplotGrob(g2), size="last"))
```

The amount of zero results has a small variation over the analyzed days and for the analyzed groups. This does not happen with the non-zero results amount, that has a large variation and will determine the increase or decreasing of the daily zero results rate (ZRR).

## [Calculation](#calc3)

```{r calc_32}
zrrs_df = searches3 %>% 
  mutate(n_results_typegroup = ifelse(results > 0, ">0", "==0")) %>% 
  group_by(day, group) %>% 
  summarise(zero_results = sum(n_results_typegroup == "==0"),
            not_zero_results = sum(n_results_typegroup == ">0"),
            sum_results = zero_results + not_zero_results,
            zrr = zero_results/sum_results * 100) 

zrrs_df %>% head()
```

```{r calc_33}
zrrs_df %>%   
  ggplot(aes(x = day, y = zrr, color=group)) +
  geom_point() +
  geom_line() +
  labs(title="Zero Results Rate by Day",
       x = "Day",
       y = "Zero Results Rate (%)",
       color = "Group") +
  ylim(0, 100)
```

It is very small the difference of the ZRR between the groups *a* and *b*, varying around 17% and 19% in the seven days of our dataset. The time and the group seems to have no relation to the zero results rate (ZRR).


# [Question 4](#question4)
####*Let session length be approximately the time between the first event and the last event in a session. Choose a variable from the dataset and describe its relationship to session length. Visualize the relationship.*

## [Cleaning Data]({#clean4})

The relationship that interest us here is the one may exist between *session_length* and *num_clicks* in a _search session_. Following the definition of *session_length*, some sessions end up having length zero once they have only one *visitPage* event associated with them. These ones will be filtered by us.

```{r clean4}
sessions <- searches %>%
  group_by(session_id) %>% 
  summarize(sum_num_clicks = sum(num_clicks),
            session_length = max(first_event_timestamp)-min(first_event_timestamp)) %>% 
  filter(session_length > 0)

sessions %>% head()
```

Next, lets take a look at some correlation tests results to these two variables (*num_clicks* and *session_length*):

```{r calc4}
cor.test(sessions$sum_num_clicks, sessions$session_length, method = "pearson")
cor.test(sessions$sum_num_clicks, sessions$session_length, method = "spearman")
cor.test(sessions$sum_num_clicks, sessions$session_length, method = "kendall")
```

As we can see the three different methods have the same result: there is no correlation between the sum of clicks in a search session and the time the session lasts.

## [Visualization](#vis4)

We can confirm the correlation tests results graphically: there is no observable tendency on the data.

```{r vis4, warning=FALSE}

summary(sessions)

sessions %>% 
  ggplot(aes(x = session_length, y = sum_num_clicks)) +
  geom_point(alpha = .3) +
  scale_x_log10() +
  scale_y_log10() +
  labs(x = "Session Length",
       y = "Number of Clicks",
       title="Number of Clicks vs Session Length")
```


